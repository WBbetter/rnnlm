<html>

<head>
<title>RNNLM Toolkit</title>
</head>

<body>

<h2>RNNLM Toolkit</h2>
by Tomas Mikolov, 2010-2012

<h3>Introduction</h3>
<p>Neural network based language models are nowdays among the most successful techniques for statistical language modeling. They can be easily applied in wide range of tasks, including automatic speech recognition and machine translation, and provide significant improvements over classic backoff n-gram models. The 'rnnlm' toolkit can be used to train, evaluate and use such models.</p>

<p>The goal of this toolkit is to speed up research progress in the language modeling field. First, by providing useful implementation that can demonstrate some of the principles. Second, for the empirical experiments when used in speech recognition and other applications. And finally third, by providing a strong state of the art baseline results, to which future research that aims to "beat state of the art techniques" should compare to.</p>

<h3>Download</h3>

<p><A href="http://www.fit.vutbr.cz/~imikolov/rnnlm/rnnlm-0.1h.tgz">rnnlm-0.1h</A> - some older version of the toolkit</p>

<p><A href="http://www.fit.vutbr.cz/~imikolov/rnnlm/rnnlm-0.2b.tgz">rnnlm-0.2b</A></p>

<p><A href="http://www.fit.vutbr.cz/~imikolov/rnnlm/rnnlm-0.2c.tgz">rnnlm-0.2c</A></p>

<p><A href="http://www.fit.vutbr.cz/~imikolov/rnnlm/rnnlm-0.3b.tgz">rnnlm-0.3b</A></p>

<p><A href="http://www.fit.vutbr.cz/~imikolov/rnnlm/rnnlm-0.3c.tgz">rnnlm-0.3c</A></p>

<p><A href="http://www.fit.vutbr.cz/~imikolov/rnnlm/rnnlm-0.3d.tgz">rnnlm-0.3d</A></p>

<p><A href="http://www.fit.vutbr.cz/~imikolov/rnnlm/rnnlm-0.3e.tgz">rnnlm-0.3e</A> - latest version of the toolkit</p>

<p><A href="http://www.fit.vutbr.cz/~imikolov/rnnlm/simple-examples.tgz">Basic examples</A> - very useful for quick introduction (training, evaluation, hyperparameter selection, simple n-best list rescoring, etc.) - 35MB</p>

<p><A href="http://www.fit.vutbr.cz/~imikolov/rnnlm/rnn-rt07-example.tar.gz">Advanced examples</A> - includes large scale experiments with speech lattices (n-best list rescoring, ...) - 235MB, by Stefan Kombrink </p>

<p>Slides from my presentation at Google - <A href="http://www.fit.vutbr.cz/~imikolov/rnnlm/google.pdf">pdf</A></p>

<p>RNNLM is now integrated into Kaldi toolkit! Check <A href="http://www.fit.vutbr.cz/~kombrink/personal/rnn-kaldi/">this</A>.

<p>Example of data generated by <A href="http://www.fit.vutbr.cz/~imikolov/rnnlm/gen-4gram.txt">4-gram</A> language model, by <A href="http://www.fit.vutbr.cz/~imikolov/rnnlm/gen-rnn-640.txt">RNN</A> model and by <A href="http://www.fit.vutbr.cz/~imikolov/rnnlm/gen-rnnme-480.txt">RNNME</A> model (all models are trained on Broadcast news data, 400M/320M words) - check which generated sentences are easier to read!</p>

<p>Word projections from <A href="http://www.fit.vutbr.cz/~imikolov/rnnlm/word_projections-80.txt.gz">RNN-80</A> and <A href="http://www.fit.vutbr.cz/~imikolov/rnnlm/word_projections-640.txt.gz">RNN-640</A> models trained on Broadcast news data + <A href="http://www.fit.vutbr.cz/~imikolov/rnnlm/distance.c">tool</A> for computing the closest words. (extra large 1600-dimensional features from 3 models are <A href="http://www.fit.vutbr.cz/~imikolov/rnnlm/word_projections-1600.txt.gz">here</A>) </p>

<h3>Frequently asked questions</h3>

<p><A href="http://www.fit.vutbr.cz/~imikolov/rnnlm/FAQ.txt">FAQ archive</A>

<h3>Contact</h3>

<p>Tomas Mikolov - tmikolov@gmail.com</p>
<p>Stefan Kombrink - kombrink@fit.vutbr.cz</p>


<h3>Acknowledgements</h3>

We would like to thank to all who have helped us with the development of this toolkit, either by providing advices or by testing it. Specially, thanks to Anoop Deoras, Sanjeev Khudanpur, Scott Novotney, Stefan Kombrink, Dan Povey, YongZhe Shi, Geoff Zweig.

<h3>References</h3>

<p><b>Mikolov Tomá¹</b>: <A href="http://www.fit.vutbr.cz/~imikolov/rnnlm/thesis.pdf">Statistical Language Models based on Neural Networks</A>. PhD thesis, Brno University of Technology, 2012.<br /><i>All the details that did not make it into the papers, more results on additional taks.</i></p>

<p><b>Mikolov Tomá¹, Sutskever Ilya, Deoras Anoop, Le Hai-Son, Kombrink Stefan, Èernocký Jan</b>: <A href="http://www.fit.vutbr.cz/~imikolov/rnnlm/char.pdf">Subword Language Modeling with Neural Networks</A>. Not published (rejected from ICASSP 2012).<br /><i>Using subwords as basic units for RNNLMs has several advantages: no OOV rate, smaller model size and better speed. Just split the infrequent words into subword units.</i></p>

<p><b>Mikolov Tomá¹, Deoras Anoop, Povey Daniel, Burget Luká¹, Èernocký Jan</b>: <A href="http://www.fit.vutbr.cz/~imikolov/rnnlm/asru_large_v4.pdf">Strategies for Training Large Scale Neural Network Language Models</A>, In: Proceedings of ASRU 2011<br /><i>How to train RNN LM on a single core on 400M words in a few days, with 1% absolute improvement in WER on state of the art setup.</i></p>

<p><b>Mikolov Tomá¹, Kombrink Stefan, Deoras Anoop, Burget Luká¹, Èernocký Jan</b>: <A href="http://www.fit.vutbr.cz/~imikolov/rnnlm/rnnlm-demo.pdf">RNNLM - Recurrent Neural Network Language Modeling Toolkit</A>, In: ASRU 2011 Demo Session<br /><i>Brief description of the RNN LM toolkit that is available on this website.</i></p>

<p><b>Mikolov Tomá¹, Deoras Anoop, Kombrink Stefan, Burget Luká¹, Èernocký Jan</b>: <A href="http://www.fit.vutbr.cz/~imikolov/rnnlm/is2011_emp.pdf">Empirical Evaluation and Combination of Advanced Language Modeling Techniques</A>, In: Proceedings of the 12th Annual Conference of the International Speech Communication Association (INTERSPEECH 2011), Florence, IT<br /><i>Comparison to other LMs shows that RNN LMs are state of the art by a large margin. Improvements inrease with more training data.</i></p>

<p><b>Kombrink Stefan, Mikolov Tomá¹, Karafiát Martin, Burget Luká¹</b>: <A href="http://www.fit.vutbr.cz/~imikolov/rnnlm/ApplicationOfRNNinMeetingRecognition_IS2011.pdf">Recurrent Neural Network based Language Modeling in Meeting Recognition</A>, In: Proceedings of the 12th Annual Conference of the International Speech Communication Association (INTERSPEECH 2011), Florence, IT<br /><i>Easy way how to adapt RNN LM + speedup tricks for rescoring (can be faster than 0.05 RT)</i></p>

<p><b>Deoras Anoop, Mikolov Tomá¹, Kombrink Stefan, Karafiát Martin, Khudanpur Sanjeev</b>: <A href="http://www.fit.vutbr.cz/research/groups/speech/publi/2011/deoras_icassp2011_5532.pdf">Variational Approximation of Long-span Language Models for LVCSR</A>, In: Proceedings of the 2011 IEEE International Conference on Acoustics, Speech, and Signal Processing, ICASSP 2011, Prague, CZ<br /><i>RNN LM can be approximated by n-gram model, and used directly in the decoder at no compuational cost.</i></p>

<p><b>Mikolov Tomá¹, Kombrink Stefan, Burget Luká¹, Èernocký Jan, Khudanpur Sanjeev</b>: <A href="http://www.fit.vutbr.cz/research/groups/speech/publi/2011/mikolov_icassp2011_5528.pdf">Extensions of Recurrent Neural Network Language Model</A>, In: Proceedings of the 2011 IEEE International Conference on Acoustics, Speech, and Signal Processing, ICASSP 2011, Prague, CZ<br /><i>Better results by using Backpropagation throught time and better speed by using classes.</i></p>

<p><b>Mikolov Tomá¹, Karafiát Martin, Burget Luká¹, Èernocký Jan, Khudanpur Sanjeev</b>: <A href="http://www.fit.vutbr.cz/research/groups/speech/publi/2010/mikolov_interspeech2010_IS100722.pdf">Recurrent neural network based language model</A>, In: Proceedings of the 11th Annual Conference of the International Speech Communication Association (INTERSPEECH 2010), Makuhari, Chiba, JP<br /><i>We show that RNN LM can be trained just by simple backpropagation, despite the popular beliefs.</i></p>

<p><A href="http://www.fit.vutbr.cz/~imikolov/rnnlm/COPYRIGHT.txt">Copyright</A>

</body>
</html>
